import sys
import os
import warnings

# ç¡®ä¿å¯¼å…¥æ­£ç¡®çš„utilsæ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from utils.util import encode_base64, prepare_qwen2_5_input, get_important_image_tokens, create_attention_mask

import torch
from os import path
from transformers import Qwen3VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
from PIL import Image
from agents.CFG import CFGLogits 
from agents.attention import qwen_modify, qwen_modify_with_importance
import torch
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
from skimage.measure import block_reduce

# é™åˆ¶å›¾ç‰‡æœ€å¤§å°ºå¯¸
pre_define_max_size=1440

# æŠ‘åˆ¶transformersç”Ÿæˆé…ç½®çš„è­¦å‘Š
warnings.filterwarnings('ignore', message='.*do_sample.*temperature.*', category=UserWarning)

QWEN = {
    'Qwen3-VL-8B': 'Qwen/Qwen3-VL-8B-Instruct'
}

# Qwen3-VL æ¨èçš„ç”Ÿæˆè¶…å‚æ•°
QWEN3_VL_GENERATION_CONFIG = {
    'do_sample': True,
    'top_p': 0.8,
    'top_k': 20,
    'temperature': 0.7,
    'repetition_penalty': 1.0,
}

ANSWER_INSTRUCTION = 'Answer given questions. If you are not sure about the answer, say you don\'t ' \
                     'know honestly. Don\'t imagine any contents that are not in the image.'

SUB_ANSWER_INSTRUCTION = 'Answer: '  # template following qwen2_5 huggingface demo


def get_chat_log(questions, answers, last_n=-1):
    n_addition_q = len(questions) - len(answers)
    assert (n_addition_q) in [0, 1]
    template = 'Question: {} \nAnswer: {} \n'
    chat_log = ''
    if last_n > 0:
        answers = answers[-last_n:]
        questions = questions[-(last_n + n_addition_q):]
    elif last_n == 0:
        answers = []
        questions = questions[-1:] if n_addition_q else []

    for i in range(len(answers)):
        chat_log = chat_log + template.format(questions[i], answers[i])
    if n_addition_q:
        chat_log = chat_log + 'Question: {}'.format(questions[-1])
    else:
        chat_log = chat_log[:-2]
    return chat_log


def trim_answer(answer):
    if isinstance(answer, list):
        return answer
    answer = answer.split('Question:')[0].replace('\n', ' ').strip()
    return answer


class MLLMBot:
    def __init__(self, model_tag, model_name, pai_enable_attn=False, device='cpu', device_id=0, bit8=False, max_answer_tokens=-1):
        self.model_tag = model_tag
        self.model_name = model_name
        self.max_answer_tokens = max_answer_tokens

        local_model_path_abs = "./models/Qwen"
        local_model_path = path.join(local_model_path_abs, QWEN[self.model_tag].split('/')[-1])

        # åŠ è½½å¤„ç†å™¨
        self.processor = AutoProcessor.from_pretrained(local_model_path)

        print("\n================= æ¨¡å‹åˆå§‹åŒ–ï¼ˆMLLMBot - Qwen3-VLï¼‰ =================")
        print(f"ğŸ“Œ æ¨¡å‹æ ‡è¯†ï¼ˆmodel_tagï¼‰: {model_tag}")
        print(f"ğŸ“Œ æ¨¡å‹åç§°ï¼ˆmodel_nameï¼‰: {model_name}")
        print(f"ğŸ“ æœ¬åœ°æ¨¡å‹è·¯å¾„: {local_model_path}")
        print(f"ğŸ“ å›¾ç‰‡æœ€å¤§å°ºå¯¸: {pre_define_max_size} ï¼Œè¶…å‡ºè¿™ä¸ªå€¼å°†å‹ç¼©")

        # ========== CPU ==========
        if device == 'cpu':
            self.device = 'cpu'
            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                local_model_path,
                torch_dtype="auto"
            )
            dtype_used = "autoï¼ˆCPU é»˜è®¤ï¼‰"
            print(f"ğŸ–¥ï¸ è®¾å¤‡: CPU")

        # ========== GPU ==========
        else:
            self.device = f'cuda:{device_id}'
            self.bit8 = bit8

            print(f"ğŸ–¥ï¸ è®¾å¤‡: GPU - {self.device}")
            print(f"ğŸ¤– ä½¿ç”¨ 8bit æ¨ç†: {'æ˜¯' if self.bit8 else 'å¦'}")

            # æŒ‰å®˜æ–¹ç¤ºä¾‹ï¼šä½¿ç”¨ bfloat16 æˆ– 8bit é‡åŒ–
            if self.bit8:
                dtype_config = {"load_in_8bit": True}
                dtype_used = "int8ï¼ˆ8bit é‡åŒ–æ¨ç†ï¼‰"
            else:
                # Qwen3-VL å®˜æ–¹æ¨èä½¿ç”¨ bfloat16
                dtype_config = {"torch_dtype": torch.bfloat16}
                dtype_used = "bfloat16ï¼ˆå®˜æ–¹æ¨èï¼‰"

            print(f"ğŸ” ä½¿ç”¨æ•°æ®ç±»å‹: {dtype_used}")

            self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                local_model_path,
                device_map="auto",
                **dtype_config
            ).eval()

            # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹
            if hasattr(self.model, 'gradient_checkpointing_enable'):
                self.model.gradient_checkpointing_enable()
                print("âœ“ å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜")

        print(f"ğŸ“ local_model_path: {local_model_path}")
        print(f"ğŸ”¢ å½“å‰ä½¿ç”¨çš„ç²¾åº¦ dtype: {dtype_used}")
        print(f"ğŸ”§ æœ€å¤§ç”Ÿæˆé•¿åº¦ max_answer_tokens: {self.max_answer_tokens}")
        print("ğŸš€ æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print("========================================================\n")
        
        # TODOè¶…å‚æ•°
        self.pai_enable_attn = pai_enable_attn   # é˜¶æ®µä¸€ï¼šæ˜¯å¦å¢å¼ºå›¾åƒæ³¨æ„åŠ›
        self.pai_alpha = 0.5           # é˜¶æ®µä¸€ï¼šå¢å¼ºç³»æ•° Î±
        self.pai_layers = (10, 28)     # é˜¶æ®µä¸€ï¼šå±‚å…ˆéªŒï¼ˆæ·±å±‚æ›´æœ‰æ•ˆï¼‰
        self.pai_enable_cfg = False    # é˜¶æ®µäºŒï¼šæ˜¯å¦å¼€å¯CFG logitsç²¾ç‚¼
        self.pai_gamma = 1.1           # é˜¶æ®µäºŒï¼šÎ³ æŒ‡å¯¼å¼ºåº¦
        self.num_map = 0
        
    def __del__(self):
        """ææ„å‡½æ•°ï¼šæ¸…ç†GPUå†…å­˜"""
        try:
            if hasattr(self, 'model'):
                del self.model
            if hasattr(self, 'processor'):
                del self.processor
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except Exception as e:
            print(f"æ¸…ç†MLLMBotå†…å­˜æ—¶å‡ºé”™: {e}")
    
    def cleanup(self):
        """æ‰‹åŠ¨æ¸…ç†å†…å­˜"""
        try:
            if hasattr(self, 'model'):
                del self.model
            if hasattr(self, 'processor'):
                del self.processor
            torch.cuda.empty_cache()
            print("MLLMBotå†…å­˜å·²æ¸…ç†")
        except Exception as e:
            print(f"æ¸…ç†MLLMBotå†…å­˜æ—¶å‡ºé”™: {e}")
        
    def _get_model_device(self):
        try:
            return self.model.model.embed_tokens.weight.device
        except Exception:
            # é€€åŒ–æ–¹æ¡ˆï¼šå–ç¬¬ä¸€ä¸ªå‚æ•°æ‰€åœ¨è®¾å¤‡æˆ– self.device
            try:
                return next(self.model.parameters()).device
            except Exception:
                return torch.device(self.device)

    # # TODO è¿™é‡Œåº”è¯¥éœ€è¦è€ƒè™‘chunkåˆ‡åˆ†
    # def _resolve_img_token_span(self, messages, inputs):
    #     """è¿”å›(img_start_idx, img_end_idx)ã€‚
    #     å¯å‘å¼ï¼šç¼ºå°‘æ˜¾å¼ image special token æ—¶ï¼Œè¿‘ä¼¼æŠŠæœ«å°¾ 256 ä¸ª token å½“ä½œå›¾åƒåŒºåŸŸã€‚
    #     è‹¥åºåˆ—è¿‡çŸ­æˆ–æ— æ³•è§£æï¼Œåˆ™è¿”å› (None, None) è·³è¿‡æ³¨å…¥ã€‚
    #     """
    #     try:
    #         input_ids = inputs.input_ids
    #         if input_ids is None:
    #             print(f'input_ids is None')
    #             return None, None
    #         seq_len = input_ids.shape[1]
    #         img_tokens = 256
    #         print(f'input_ids:{input_ids.shape}\nseq_len:{seq_len}')
    #         if seq_len <= img_tokens:
    #             print(f'seq_len <= img_tokens')
    #             return None, None
    #         img_start = seq_len - img_tokens
    #         img_end = seq_len
    #         print(f'img_start:{img_start}, img_end:{img_end}')
    #         return img_start, img_end
    #     except Exception as e:
    #         print(f"error return None None:{e}")
    #         return None, None


    def _resolve_img_token_span(self, messages, inputs):
        try:
            input_ids = inputs.input_ids
            if input_ids is None:
                print(f'input_ids is None')
                return None, None
            seq_len = input_ids.shape[1]
            # tokenizer é‡Œæœ‰ special token çš„æ˜ å°„
            tokenizer = self.processor.tokenizer
            vision_start_id = tokenizer.convert_tokens_to_ids('<|vision_start|>')
            image_pad_id = tokenizer.convert_tokens_to_ids('<|image_pad|>')
            vision_end_id = tokenizer.convert_tokens_to_ids('<|vision_end|>')
            print(f'input_ids:{input_ids.shape}\nseq_len:{seq_len}')
            input_ids_list = input_ids[0].tolist()
            if vision_start_id in input_ids_list and vision_end_id in input_ids_list:
                img_start = input_ids_list.index(vision_start_id)
                img_end   = input_ids_list.index(vision_end_id) + 1  # åŒ…å« img_end
                print(f"æ‰¾åˆ° image token span: img_start={img_start}, img_end={img_end}")
                return img_start, img_end
            else:
                print("æœªæ‰¾åˆ° image token span")
                return None, None
        except Exception as e:
            print(f"error return None None:{e}")
            return None, None

    def _inject_qwen_pai_attention(self, img_start_idx, img_end_idx):
        if img_start_idx is None or img_end_idx is None:
            print('[ATTN] skip injection for Qwen3 (img span unresolved).')
            return
        print(f'[ATTN] inject Qwen3 attention layers {self.pai_layers} alpha={self.pai_alpha} span=({img_start_idx},{img_end_idx})')
        qwen_modify(self.model, self.pai_layers[0], self.pai_layers[1], True, self.pai_alpha, False, img_start_idx, img_end_idx)

    def _inject_qwen_pai_attention_with_importance(self, img_start_idx, img_end_idx, important_tokens_info):
        if img_start_idx is None or img_end_idx is None:
            print('[ATTN] skip injection for Qwen3 (img span unresolved).')
            return
        
        print(f'[ATTN] inject Qwen3 attention layers with importance weights {self.pai_layers} alpha={self.pai_alpha} span=({img_start_idx},{img_end_idx})')
        
        # æå–é‡è¦æ€§æƒé‡ä¿¡æ¯
        importance_weights = important_tokens_info['weights']  # æ‰€æœ‰å›¾åƒtokençš„æƒé‡
        important_indices = important_tokens_info['important_indices']  # é‡è¦tokençš„ç´¢å¼•
        
        # è°ƒç”¨ä¿®æ”¹å‡½æ•°ï¼Œä¼ é€’é‡è¦æ€§ä¿¡æ¯
        qwen_modify_with_importance(self.model, self.pai_layers[0], self.pai_layers[1], True, self.pai_alpha, False, img_start_idx, img_end_idx, importance_weights, important_indices)

    def get_name(self):
        return self.model_name
    
    def _resize_image_if_needed(self, image: Image.Image, max_size: int = pre_define_max_size) -> Image.Image:
        """
        å¦‚æœå›¾åƒå°ºå¯¸è¶…è¿‡max_sizeï¼ŒæŒ‰æ¯”ä¾‹ç¼©å°ä»¥é˜²æ­¢æ˜¾å­˜çˆ†ç‚¸
        
        Args:
            image: PILå›¾åƒ
            max_size: æœ€å¤§è¾¹é•¿ï¼ˆé»˜è®¤pre_define_max_sizeï¼Œé¢„å®šä¹‰å¥½ï¼‰
            
        Returns:
            è°ƒæ•´åçš„PILå›¾åƒ
        """
        width, height = image.size
        max_dim = max(width, height)
        
        if max_dim > max_size:
            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            scale = max_size / max_dim
            new_width = int(width * scale)
            new_height = int(height * scale)
            
            print(f"å›¾åƒè¿‡å¤§ ({width}x{height})ï¼Œç¼©å°åˆ° ({new_width}x{new_height}) ä»¥èŠ‚çœæ˜¾å­˜")
            return image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        return image

    def __call_qwen3(self, raw_image, prompt, max_new_tokens=256):
        """
        Qwen3-VL æ¨ç†æ–¹æ³•ï¼ŒåŸºäºå®˜æ–¹ç¤ºä¾‹å®ç°
        """
        if isinstance(raw_image, Image.Image):
            raw_image = [raw_image]

        # æ„å»º content åˆ—è¡¨
        content = []
        for img in raw_image:
            # é™åˆ¶å›¾åƒæœ€å¤§å°ºå¯¸ï¼Œé˜²æ­¢è¶…å¤§å›¾ç‰‡å¯¼è‡´æ˜¾å­˜çˆ†ç‚¸
            img = self._resize_image_if_needed(img, max_size=pre_define_max_size)
            # ç›´æ¥ä¼  PIL Image å¯¹è±¡ï¼Œè€Œä¸æ˜¯ base64 å­—ç¬¦ä¸²
            content.append({"type": "image", "image": img})
        content.append({"type": "text", "text": prompt})
        
        # æ„é€  messagesï¼ˆå®˜æ–¹æ ¼å¼ï¼‰
        messages = [
            {
                "role": "user",
                "content": content
            }
        ]
        
        # ä½¿ç”¨å®˜æ–¹æ¨èçš„ apply_chat_template æ–¹æ³•å‡†å¤‡è¾“å…¥
        inputs = self.processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        # ç§»åŠ¨åˆ°æ¨¡å‹è®¾å¤‡
        model_device = self._get_model_device()
        inputs = inputs.to(model_device)

        # TODO: æ³¨æ„åŠ›å¢å¼ºåŠŸèƒ½æš‚æ—¶ç¦ç”¨ï¼ŒQwen3-VL éœ€è¦é€‚é…
        # if self.pai_enable_attn:
        #     pass

        # æ¸…ç†æ˜¾å­˜ç¼“å­˜
        torch.cuda.empty_cache()
        
        # æ£€æŸ¥æ˜¾å­˜ä½¿ç”¨æƒ…å†µ - åŸºäºå‰©ä½™æ˜¾å­˜çš„æ¸…ç†ç­–ç•¥
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            memory_reserved = torch.cuda.memory_reserved() / 1024**3   # GB
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            memory_free = memory_total - memory_allocated
            print(f"æ¨ç†å‰æ˜¾å­˜: å·²åˆ†é…={memory_allocated:.2f}GB, å·²ä¿ç•™={memory_reserved:.2f}GB, å‰©ä½™={memory_free:.2f}GB")
            
            # å¦‚æœå‰©ä½™æ˜¾å­˜ < 12GBï¼Œè§¦å‘æ¸…ç†ï¼ˆé€‚é…A6000 48GBå’ŒA800 80GBï¼‰
            if memory_free < 12:  
                print(f"è­¦å‘Š: å‰©ä½™æ˜¾å­˜ä¸è¶³ ({memory_free:.2f}GB < 12GB)ï¼Œå¼ºåˆ¶æ¸…ç†...")
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                # å†æ¬¡æ£€æŸ¥
                memory_after_clear = torch.cuda.memory_allocated() / 1024**3
                memory_free_after = memory_total - memory_after_clear
                print(f"æ¸…ç†åå‰©ä½™æ˜¾å­˜: {memory_free_after:.2f}GB")
                if memory_free_after < 10:
                    torch.cuda.reset_peak_memory_stats()
                    print(f"å·²é‡ç½®å³°å€¼æ˜¾å­˜ç»Ÿè®¡")
        
        with torch.no_grad():
            # ä½¿ç”¨ Qwen3-VL æ¨èçš„ç”Ÿæˆå‚æ•°
            generation_config = QWEN3_VL_GENERATION_CONFIG.copy()
            generation_config.update({
                'max_new_tokens': max_new_tokens,
                'use_cache': True,
                'pad_token_id': self.processor.tokenizer.eos_token_id,
            })
            
            # å®˜æ–¹ç¤ºä¾‹çš„ç”Ÿæˆæ–¹å¼
            generated_ids = self.model.generate(
                **inputs,
                **generation_config
            )
            
        # æŒ‰å®˜æ–¹ç¤ºä¾‹å¤„ç†è¾“å‡º
        generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        # è§£ç è¾“å‡º
        reply = self.processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )
        
        # æ¸…ç†ä¸´æ—¶å˜é‡
        del inputs, generated_ids, generated_ids_trimmed
        
        # åŸºäºå‰©ä½™æ˜¾å­˜å†³å®šæ˜¯å¦æ¸…ç†
        if torch.cuda.is_available():
            memory_after = torch.cuda.memory_allocated() / 1024**3
            memory_reserved = torch.cuda.memory_reserved() / 1024**3
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            memory_free = memory_total - memory_after
            print(f"æ¨ç†åæ˜¾å­˜: å·²åˆ†é…={memory_after:.2f}GB, å·²ä¿ç•™={memory_reserved:.2f}GB, å‰©ä½™={memory_free:.2f}GB")
            
            # åªåœ¨å‰©ä½™æ˜¾å­˜ < 10GB æ—¶æ‰æ¸…ç†ï¼Œå¦åˆ™ä¿ç•™ç¼“å­˜æå‡æ€§èƒ½
            if memory_free < 10:
                torch.cuda.empty_cache()
                print(f"å‰©ä½™æ˜¾å­˜ä¸è¶³10GBï¼Œå·²æ¸…ç†ç¼“å­˜")
                
                # å¦‚æœä¿ç•™çš„æ˜¾å­˜è¿‡å¤šä¸”å‰©ä½™ä¸è¶³10GBï¼Œé‡ç½®å³°å€¼ç»Ÿè®¡
                if memory_reserved > memory_free and memory_free < 10:
                    torch.cuda.reset_peak_memory_stats()
                    print(f"å·²é‡ç½®å³°å€¼æ˜¾å­˜ç»Ÿè®¡")
        
        # print(f"test MLLM answer after decode: {reply}")
        return reply

    def answer_chat_log(self, raw_image, chat_log, n_context=-1):
        # prepare the context for qwen3
        qwen3_prompt = '\n'.join([ANSWER_INSTRUCTION,
                                  get_chat_log(chat_log['questions'],chat_log['answers'],
                                               last_n=n_context), SUB_ANSWER_INSTRUCTION]
                                 )

        reply = self.__call_qwen3(raw_image, qwen3_prompt)
        trimmed_reply = trim_answer(reply)
        return reply, trimmed_reply

    def tell_me_the_obj(self, raw_image, super_class, super_unit):
        std_prompt = f"Questions: What is the {super_unit} of the {super_class} in this photo? Answer:"
        # std_prompt = f"Questions: What is the name of the main object in this photo? Answer:"
        reply = self.__call_qwen3(raw_image, std_prompt)
        trimmed_reply = trim_answer(reply)
        return reply, trimmed_reply

    def describe_attribute(self, raw_image, attr_prompt, max_new_tokens=256):
        # raw_imageæ˜¯Image.openä¹‹åçš„æ ¼å¼   
        reply = self.__call_qwen3(raw_image, attr_prompt, max_new_tokens)
        trimmed_reply = trim_answer(reply)
        return reply, trimmed_reply
    
    def compare_attention_enhancement(self, raw_image, attr_prompt, save_dir="./experiments/attention_comparison"):
        """
        å¯¹æ¯”æ³¨æ„åŠ›å¢å¼ºå‰åçš„æ•ˆæœ
        """
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        print("=" * 60)
        print("ATTENTION ENHANCEMENT COMPARISON")
        print("=" * 60)
        
        # 1. è¿è¡Œæœªå¢å¼ºç‰ˆæœ¬
        print("\n[1] Running WITHOUT attention enhancement...")
        original_attn = self.pai_enable_attn
        self.pai_enable_attn = False
        
        reply_no_enhance, _ = self.describe_attribute(raw_image, attr_prompt)
        print(f"Without enhancement: {reply_no_enhance}")
        
        # 2. è¿è¡Œå¢å¼ºç‰ˆæœ¬
        print("\n[2] Running WITH attention enhancement...")
        self.pai_enable_attn = True
        
        reply_with_enhance, _ = self.describe_attribute(raw_image, attr_prompt)
        print(f"With enhancement: {reply_with_enhance}")
        
        # 3. æ¢å¤åŸå§‹è®¾ç½®
        self.pai_enable_attn = original_attn
        
        # 4. ä¿å­˜å¯¹æ¯”ç»“æœ
        with open(os.path.join(save_dir, "comparison_results.txt"), "w", encoding="utf-8") as f:
            f.write("ATTENTION ENHANCEMENT COMPARISON\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Prompt: {attr_prompt}\n\n")
            f.write(f"Without enhancement: {reply_no_enhance}\n\n")
            f.write(f"With enhancement: {reply_with_enhance}\n\n")
            f.write(f"Enhancement layers: {self.pai_layers}\n")
            f.write(f"Alpha value: {self.pai_alpha}\n")
        
        print(f"\n[3] Comparison results saved to {save_dir}")
        print("=" * 60)
        
        return reply_no_enhance, reply_with_enhance

    def caption(self, raw_image):
        # standard way to caption an image in the qwen3 paper
        std_prompt = 'a photo of'
        reply = self.__call_qwen3(raw_image, std_prompt)
        reply = reply[0] if isinstance(reply, list) else reply
        reply = reply.replace('\n', ' ').strip()  # trim caption
        return reply

    def call_llm(self, prompts):
        prompts_temp = self.processor(None, prompts, return_tensors="pt")
        model_device = self._get_model_device()
        input_ids = prompts_temp['input_ids'].to(model_device)
        attention_mask = prompts_temp['attention_mask'].to(model_device)

        prompts_embeds = self.model.language_model.get_input_embeddings()(input_ids)

        with torch.no_grad():
            outputs = self.model.language_model.generate(
                inputs_embeds=prompts_embeds,
                attention_mask=attention_mask)

        outputs = self.processor.decode(outputs[0], skip_special_tokens=True)
        return outputs
