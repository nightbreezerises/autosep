import os
import sys
import warnings
from google import genai
from google.genai.errors import APIError
from typing import List, Dict, Any, Union
from PIL import Image

# ----------------------------------------------------------------------
# âš ï¸ æ³¨æ„: Gemini æ˜¯ä¸€ä¸ªäº‘ç«¯ API æœåŠ¡ï¼Œä¸ Qwen çš„æœ¬åœ° MLLM æ¨ç†æ¨¡å¼å®Œå…¨ä¸åŒã€‚
# ä»¥ä¸‹ä»£ç å·²ç§»é™¤æ‰€æœ‰æœ¬åœ°æ¨¡å‹åŠ è½½ã€GPU/CPUã€PyTorch/Transformersç›¸å…³çš„é€»è¾‘ã€‚
# ä»…ä¿ç•™ API å®¢æˆ·ç«¯åˆå§‹åŒ–å’Œè°ƒç”¨é€»è¾‘ï¼Œå¹¶éµå¾ªæ‚¨æä¾›çš„ MLLMBot ç»“æ„ã€‚
# ----------------------------------------------------------------------

proxy_host = "127.0.0.1"
proxy_port = 27376  # ä»£ç†ç«¯å£ï¼Œæ–¹ä¾¿ç”¨æˆ·æ›´æ”¹
# ç­‰ä»·äº: export http_proxy="http://127.0.0.1:PORT" https_proxy="http://127.0.0.1:PORT"
# ä»…éœ€ä¿®æ”¹ proxy_port å³å¯åˆ‡æ¢ç«¯å£ï¼Œé»˜è®¤ä»£ç†æ§åˆ¶å°: 10.82.1.223:19136/ui
print(f"[Gemini Debug] å½“å‰é»˜è®¤ä»£ç†: http://{proxy_host}:{proxy_port}")
print("[Gemini Debug] å¦‚éœ€ä¿®æ”¹ï¼Œè¯·åœ¨æ–‡ä»¶é¡¶éƒ¨è°ƒæ•´ proxy_port æˆ– proxy_host åé‡æ–°è¿è¡Œã€‚")

# æŠ‘åˆ¶ warnings
warnings.filterwarnings('ignore', category=UserWarning)

# æ˜ å°„ Gemini æ¨¡å‹åç§°
GEMINI_MODELS = {
    'gemini-2.5-pro': 'gemini-2.5-pro', # å¼ºå¤§çš„æ¨¡å‹
    'gemini-2.5-flash': 'gemini-2.5-flash', # é«˜é€Ÿ/ä½å»¶è¿Ÿæ¨¡å‹
    # ä¹Ÿå¯ä»¥æ·»åŠ å…¶ä»–ç‰ˆæœ¬ï¼Œä¾‹å¦‚ï¼š
    # 'gemini-1.5-pro': 'gemini-1.5-pro',
}

api_key = "AIzaSyAjtpXhIfF_y-RvTNFTDNocOTB7hhQ4l6s"

SYSTEM_INSTRUCTION = "You are a helpful assistant."

# Gemini API çš„é‡è¯•é€»è¾‘å¯ä»¥é›†æˆåœ¨è°ƒç”¨å‡½æ•°å†…éƒ¨æˆ–å¤–éƒ¨ï¼Œè¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€å•ç‰ˆæœ¬ã€‚
# Google GenAI SDK æœ¬èº«é€šå¸¸ä¼šå¤„ç†ç½‘ç»œçº§çš„é‡è¯•ï¼Œä½†æˆ‘ä»¬å¯ä»¥åœ¨åº”ç”¨å±‚æ·»åŠ é€»è¾‘ã€‚
# ç”±äºå»é™¤äº† tenacity åº“ä¾èµ–ï¼Œè¿™é‡Œä½¿ç”¨ç®€å•çš„ try/except å¾ªç¯å®ç°é‡è¯•ã€‚
MAX_RETRIES = 3


def _setup_proxy_env():
    def _clear_socks_env():
        removed = False
        for key in ("all_proxy", "ALL_PROXY", "socks_proxy", "SOCKS_PROXY"):
            val = os.environ.pop(key, None)
            if val:
                removed = True
                print(f"[Gemini Debug] å·²ç§»é™¤ {key}={val} (é¿å…è§¦å‘ SOCKS ä»£ç†)")
        return removed

    if not proxy_port:
        _clear_socks_env()
        for key in ("http_proxy", "https_proxy", "HTTP_PROXY", "HTTPS_PROXY"):
            if key in os.environ:
                os.environ.pop(key)
                print(f"[Gemini Debug] å·²æ¸…ç† {key}ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤ç½‘ç»œ")
        print("[Gemini Debug] æœªè®¾ç½®ä»£ç†ç«¯å£ï¼Œç›´æ¥è¿æ¥ Gemini API")
        return

    proxy_url = f"http://{proxy_host}:{proxy_port}"
    _clear_socks_env()

    for key in ("http_proxy", "https_proxy", "HTTP_PROXY", "HTTPS_PROXY"):
        current = os.environ.get(key)
        if current != proxy_url:
            os.environ[key] = proxy_url

    print("[Gemini Debug] å·²é…ç½®ä»£ç†ç¯å¢ƒå˜é‡:")
    print(f"  - export http_proxy=\"{proxy_url}\"")
    print(f"  - export https_proxy=\"{proxy_url}\"")


def prepare_gemini_message(main_prompt: str) -> List[Dict[str, str]]:
    """
    ä¸º Gemini API å‡†å¤‡æ¶ˆæ¯æ ¼å¼ã€‚
    Gemini èŠå¤© API ä½¿ç”¨ List[Content] ç»“æ„ï¼Œæ¯ä¸ª Content åŒ…å« role å’Œ partsã€‚
    è¿™é‡Œç®€åŒ–ä¸ºå•è½®çš„ system/user ç»“æ„ï¼Œç±»ä¼¼äº Qwen çš„ chat log æ„é€ ã€‚
    """
    messages = [
        {
            "role": "user",
            "parts": [
                {"text": f"SYSTEM_INSTRUCTION: {SYSTEM_INSTRUCTION}\n{main_prompt}"}
            ]
        }
    ]
    # å®˜æ–¹æ¨èçš„ç³»ç»ŸæŒ‡ä»¤ä¼ é€’æ–¹å¼æ˜¯åœ¨ Client æˆ– Config ä¸­ï¼Œè¿™é‡Œä¸ºäº†ä»¿ç…§æ—§ä»£ç ç»“æ„ï¼Œ
    # æš‚æ—¶å°†ç³»ç»ŸæŒ‡ä»¤æ”¾å…¥ç”¨æˆ·æç¤ºä¸­ï¼Œæˆ–è€…åœ¨ __call_llm ä¸­ä½¿ç”¨ system_instruction å‚æ•°ã€‚
    return main_prompt # è¿”å›åŸå§‹ promptï¼Œåœ¨è°ƒç”¨æ—¶å¤„ç†ç»“æ„


def trim_answer(answer: Union[str, List[str]]) -> str:
    """
    æ¸…ç†å’Œä¿®å‰ªæ¨¡å‹çš„å›å¤ã€‚
    """
    if isinstance(answer, list):
        # å…¼å®¹ __call_llm è¿”å›åˆ—è¡¨çš„æƒ…å†µ
        answer = answer[0]
        
    # ç§»é™¤å¯èƒ½çš„åˆ†éš”ç¬¦æˆ–å¤šä½™å†…å®¹
    answer = answer.split('Question:')[0].replace('\n', ' ').strip()
    return answer


class MLLMBot:
    # ä»¿ç…§ Qwen çš„ MLLMBot ç»“æ„ï¼Œä½†ä¸“æ³¨äº API è°ƒç”¨
    def __init__(self, model_tag: str, model_name: str, device: str = 'api', max_answer_tokens: int = 256):
        
        self.model_tag = model_tag # å¦‚ 'gemini-2.5-pro'
        self.model_name = model_name # å¦‚ 'Gemini 2.5 Pro (API)'
        self.max_answer_tokens = max_answer_tokens # å¯¹åº” max_output_tokens
        self.device = device
        self.total_requests = 0
        self.total_prompt_tokens = 0
        self.total_output_tokens = 0

        if self.model_tag not in GEMINI_MODELS:
            raise ValueError(f"Model tag '{model_tag}' not supported. Available: {list(GEMINI_MODELS.keys())}")
        
        # ç¡®ä¿ API Key å·²è®¾ç½®
        if not api_key:
            raise EnvironmentError("GEMINI_API_KEY environment variable is not set.")

        _setup_proxy_env()

        # åˆå§‹åŒ– Gemini å®¢æˆ·ç«¯
        self.client = genai.Client(api_key=api_key)
        self.api_model_name = GEMINI_MODELS[self.model_tag]
        
        # è®°å½•é…ç½®ä¿¡æ¯
        print("\n================= æ¨¡å‹åˆå§‹åŒ–ï¼ˆMLLMBot - Gemini APIï¼‰ =================")
        print(f"ğŸ“Œ æ¨¡å‹æ ‡è¯†ï¼ˆmodel_tagï¼‰: {model_tag}")
        print(f"ğŸ“Œ æ¨¡å‹åç§°ï¼ˆmodel_nameï¼‰: {model_name}")
        print(f"ğŸ–¥ï¸ è®¾å¤‡: {self.device} (äº‘ç«¯ API)")
        print(f"ğŸ”§ æœ€å¤§ç”Ÿæˆé•¿åº¦ max_answer_tokens: {self.max_answer_tokens}")
        print("ğŸš€ å®¢æˆ·ç«¯åŠ è½½å®Œæˆï¼")
        print("========================================================\n")
        
        # API æœåŠ¡æ²¡æœ‰ GPU/CPU æ¸…ç†ï¼Œä½†ä¿ç•™æ–¹æ³•ç­¾åä»¥ä»¿ç…§ Qwen é£æ ¼
        
    def __del__(self):
        """ææ„å‡½æ•°ï¼ˆAPIæ¨¡å¼ä¸‹æ— å®é™…æ“ä½œï¼‰"""
        pass
    
    def cleanup(self):
        """æ‰‹åŠ¨æ¸…ç†å†…å­˜ï¼ˆAPIæ¨¡å¼ä¸‹æ— å®é™…æ“ä½œï¼‰"""
        pass
        
    def get_name(self):
        return self.model_name
    
    def _log_image_debug(self, images: List[Image.Image]):
        if not images:
            print("[Gemini Debug] æ— å›¾åƒè¾“å…¥ï¼ŒæŒ‰çº¯æ–‡æœ¬æ¨¡å¼æ¨ç†")
            return
        details = []
        for idx, img in enumerate(images):
            if isinstance(img, Image.Image):
                width, height = img.size
                details.append(f"#{idx + 1}:{width}x{height},mode={img.mode}")
            else:
                details.append(f"#{idx + 1}:éPILå¯¹è±¡({type(img)})")
        print(f"[Gemini Debug] æ¥æ”¶åˆ° {len(images)} å¼ å›¾åƒ -> {' | '.join(details)}")

    def _log_prompt_debug(self, prompt: str, max_new_tokens: int):
        prompt_clean = ' '.join(prompt.strip().split())
        preview = (prompt_clean[:200] + '...') if len(prompt_clean) > 200 else prompt_clean
        print("[Gemini Debug] æ–‡æœ¬æç¤ºä¿¡æ¯ï¼š")
        print(f"  - å­—ç¬¦æ•°: {len(prompt)}")
        print(f"  - Max New Tokens: {max_new_tokens}")
        print(f"  - Preview: {preview}")

    def _log_api_response(self, response, total_tokens: int):
        usage = getattr(response, 'usage_metadata', None)
        prompt_tokens = getattr(usage, 'prompt_token_count', 0) if usage else 0
        # candidates_token_count å¯èƒ½ä¸º Noneï¼Œéœ€è¦å¤„ç†
        completion_tokens = getattr(usage, 'candidates_token_count', None) if usage else 0
        if completion_tokens is None:
            completion_tokens = 0
        # Gemini 2.5 Flash ä¼šä½¿ç”¨æ€è€ƒ token
        thoughts_tokens = getattr(usage, 'thoughts_token_count', 0) if usage else 0
        if thoughts_tokens is None:
            thoughts_tokens = 0
        candidates = getattr(response, 'candidates', [])
        finish_reason = candidates[0].finish_reason if candidates else 'unknown'
        response_text = response.text if response.text else ""
        reply_preview = response_text.strip().replace('\n', ' ')
        if len(reply_preview) > 200:
            reply_preview = reply_preview[:200] + '...'
        print("[Gemini Debug] APIè°ƒç”¨æˆåŠŸï¼š")
        print(f"  - ä½¿ç”¨æ¨¡å‹: {self.api_model_name}")
        print(f"  - Tokens Used (total/prompt/output/thoughts): {total_tokens}/{prompt_tokens}/{completion_tokens}/{thoughts_tokens}")
        print(f"  - Finish Reason: {finish_reason}")
        print(f"  - Response Preview: {reply_preview}")
        self.total_requests += 1
        self.total_prompt_tokens += prompt_tokens
        self.total_output_tokens += completion_tokens
        self._log_usage_summary()

    def _log_usage_summary(self):
        print("[Gemini Debug] ç´¯è®¡ç”¨é‡ç»Ÿè®¡ï¼š")
        print(f"  - æ€»è¯·æ±‚æ¬¡æ•°: {self.total_requests}")
        print(f"  - ç´¯è®¡ Prompt Tokens: {self.total_prompt_tokens}")
        print(f"  - ç´¯è®¡ Output Tokens: {self.total_output_tokens}")

    def _log_quota_warning(self, error: Exception):
        error_str = str(error)
        lower_err = error_str.lower()
        quota_keywords = ["insufficient_quota", "quota", "rate limit", "exceeded"]
        if any(keyword in lower_err for keyword in quota_keywords):
            print("âš ï¸ [Gemini Debug] å¯èƒ½è§¦å‘é…é¢/é€Ÿç‡é™åˆ¶ï¼Œè¯·æ£€æŸ¥ Google AI Studio ä¸­çš„ä½¿ç”¨é¢åº¦ã€‚")
            print("  - å»ºè®®: å‡å°‘ batch å¤§å°ã€é™ä½ max_output_tokensï¼Œæˆ–å‡çº§è´¦å·é¢åº¦ã€‚")

    # ä»¿ç…§ Qwen çš„ __call_qwen2_5 æ–¹æ³•
    def __call_llm(self, raw_image: Union[Image.Image, None], prompt: str, max_new_tokens: int = 256) -> List[str]:
        
        contents = []
        image_payload: List[Image.Image] = []
        
        # 1. å¤„ç†å›¾åƒ (å¤šæ¨¡æ€è¾“å…¥)
        if raw_image:
            # å…¼å®¹å•å›¾å’Œå¤šå›¾ï¼Œè¿™é‡Œå‡è®¾ raw_image æ˜¯ PIL.Image æˆ– PIL.Image åˆ—è¡¨
            if not isinstance(raw_image, list):
                raw_image = [raw_image]

            for img in raw_image:
                # Gemini API ç›´æ¥æ¥å— PIL Image å¯¹è±¡ä½œä¸º parts
                image_payload.append(img)

        self._log_image_debug(image_payload)

        contents.extend(image_payload)
        # 2. å¤„ç†æ–‡æœ¬ Prompt
        contents.append(prompt)
        self._log_prompt_debug(prompt, max_new_tokens)
        
        # 3. é…ç½®ç”Ÿæˆå‚æ•° (å¯¹åº” Qwen çš„ generate å‚æ•°)
        # æ³¨æ„: Gemini 2.5 Flash ä½¿ç”¨æ€è€ƒ tokenï¼Œéœ€è¦å¢åŠ  max_output_tokens
        # æ€è€ƒ token å¯èƒ½å ç”¨ 1000+ tokensï¼Œæ‰€ä»¥éœ€è¦å¤§å¹…é¢„ç•™ç©ºé—´
        # ä¾‹å¦‚ï¼šmax_new_tokens=256 æ—¶ï¼Œæ€è€ƒå¯èƒ½ç”¨ 1000+ï¼Œå®é™…è¾“å‡ºéœ€è¦ 256ï¼Œæ€»å…±éœ€è¦ 1500+
        effective_max_tokens = max(max_new_tokens + 2048, 2560)  # é¢„ç•™ 2048 ç»™æ€è€ƒï¼Œè‡³å°‘ 2560
        config = {
            "max_output_tokens": effective_max_tokens,
            "temperature": 0.9, # é»˜è®¤å€¼ï¼Œå¦‚æœéœ€è¦å¯ä½œä¸ºå‚æ•°ä¼ å…¥
            "system_instruction": SYSTEM_INSTRUCTION # æ¨èçš„ç³»ç»ŸæŒ‡ä»¤ä¼ é€’æ–¹å¼
        }
        
        # 4. æ‰§è¡Œ API è°ƒç”¨ (å¸¦é‡è¯•é€»è¾‘)
        reply = [""]
        total_tokens = 0
        print(f"[Gemini Debug] å³å°†è°ƒç”¨äº‘ç«¯æ¨¡å‹ {self.api_model_name}ï¼Œæœ€å¤šé‡è¯• {MAX_RETRIES} æ¬¡")
        
        for attempt in range(MAX_RETRIES):
            try:
                print(f"[Gemini Debug] ç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼Œå‘é€è¯·æ±‚...")
                response = self.client.models.generate_content(
                    model=self.api_model_name,
                    contents=contents,
                    config=config
                )
                
                # æå–å›å¤å’ŒTokenæ•°
                response_text = response.text
                if response_text is None:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å€™é€‰å†…å®¹
                    candidates = getattr(response, 'candidates', [])
                    if candidates and hasattr(candidates[0], 'content'):
                        parts = getattr(candidates[0].content, 'parts', [])
                        if parts and hasattr(parts[0], 'text'):
                            response_text = parts[0].text
                    if response_text is None:
                        print(f"[Gemini Debug] è­¦å‘Š: API è¿”å›ç©ºå†…å®¹ï¼Œå¯èƒ½è¢«å®‰å…¨è¿‡æ»¤")
                        print(f"[Gemini Debug] å“åº”å¯¹è±¡: {response}")
                        response_text = ""
                
                reply = [response_text]
                total_tokens = response.usage_metadata.total_token_count if response.usage_metadata else 0
                
                # æ‰“å° Token ç»Ÿè®¡ (ä»¿ç…§ Qwen æ‰“å°å†…å­˜/Token)
                self._log_api_response(response, total_tokens)
                break # æˆåŠŸï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                
            except APIError as e:
                print(f"API Error (Attempt {attempt + 1}/{MAX_RETRIES}): {e}")
                self._log_quota_warning(e)
                if attempt < MAX_RETRIES - 1:
                    # ä»…åœ¨éæœ€åä¸€æ¬¡å°è¯•æ—¶ç­‰å¾…
                    import time
                    time.sleep(2 ** attempt)
                else:
                    raise
            except Exception as e:
                print(f"Unexpected Error: {e}")
                raise

        # API æ¨¡å¼ä¸‹æ— æ³•è·å– total_tokens çš„å¢é‡ï¼Œè¿™é‡Œè¿”å›ä¸€ä¸ªå…ƒç»„ï¼Œä»¿ç…§å¤æ‚è°ƒç”¨çš„ä¹ æƒ¯ã€‚
        return reply, total_tokens
    

    # ä»¥ä¸‹æ–¹æ³•ä»¿ç…§ Qwen MLLMBot çš„å¤–éƒ¨æ¥å£ï¼Œè°ƒç”¨ __call_llm

    def answer_chat_log(self, raw_image, chat_log: Dict[str, List[str]], n_context: int = -1):
        """
        å¤„ç†èŠå¤©å†å²è®°å½•ï¼Œå¹¶ç”Ÿæˆå›å¤ã€‚
        """
        # âš ï¸ æ³¨æ„: Gemini API çš„å¤šè½®èŠå¤©æœ€å¥½ä½¿ç”¨ client.chats.create()ã€‚
        # è¿™é‡Œä¸ºäº†ä»¿ç…§ Qwen çš„ get_chat_log ç»“æ„ï¼Œæˆ‘ä»¬å°†å…¶æ‰“åŒ…æˆä¸€ä¸ªé•¿æ–‡æœ¬ promptã€‚
        
        # ä»¿ç…§ Qwen çš„ get_chat_log æ„é€ æ–‡æœ¬
        history_str = self._format_chat_log(chat_log, n_context)
        
        gemini_prompt = '\n'.join([
            history_str, 
            "Please provide a concise answer to the last question."
        ])

        reply_list, _ = self.__call_llm(raw_image, gemini_prompt, max_new_tokens=self.max_answer_tokens)
        
        reply = reply_list[0]
        trimmed_reply = trim_answer(reply)
        return reply, trimmed_reply

    def describe_attribute(self, raw_image: Union[Image.Image, List[Image.Image]], attr_prompt: str, max_new_tokens: int = 256):
        """
        æè¿°å›¾åƒå±æ€§æˆ–å›ç­”é—®é¢˜ã€‚
        """
        reply_list, _ = self.__call_llm(raw_image, attr_prompt, max_new_tokens)
        trimmed_reply = trim_answer(reply_list)
        return reply_list[0], trimmed_reply

    def _format_chat_log(self, chat_log: Dict[str, List[str]], last_n: int = -1) -> str:
        """
        ä»¿ç…§ Qwen çš„ get_chat_log é€»è¾‘ï¼Œå°†èŠå¤©å†å²è½¬æ¢ä¸ºæ–‡æœ¬ã€‚
        """
        questions = chat_log.get('questions', [])
        answers = chat_log.get('answers', [])
        
        n_addition_q = len(questions) - len(answers)
        
        # æˆªæ–­é€»è¾‘
        if last_n > 0:
            answers = answers[-last_n:]
            questions = questions[-(last_n + n_addition_q):]
        elif last_n == 0:
            answers = []
            questions = questions[-1:] if n_addition_q else []

        template = 'User: {} \nAssistant: {} \n'
        chat_log_str = ''

        for i in range(len(answers)):
            chat_log_str += template.format(questions[i], answers[i])
            
        if n_addition_q:
            chat_log_str += 'User: {}'.format(questions[-1])
        else:
            # ç§»é™¤æœ«å°¾çš„æ¢è¡Œå’Œç©ºæ ¼
            chat_log_str = chat_log_str.strip() 

        return chat_log_str


# ----------------------------------------------------------------------
# ç¤ºä¾‹ç”¨æ³• (ä»¿ç…§ test_get_llm_output)
# ----------------------------------------------------------------------
def test_mllm_output():
    
    # âš ï¸ è¯·ç¡®ä¿æ‚¨å·²è®¾ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡ï¼Œä¸” PIL åº“å·²å®‰è£…
    # pip install Pillow
    
    try:
        from PIL import Image
        
        # å‡†å¤‡ä¸€ä¸ªè™šæ‹Ÿå›¾åƒå¯¹è±¡ (Gemini APIæ”¯æŒå¤šæ¨¡æ€)
        # å®é™…ä½¿ç”¨ä¸­ï¼Œä½ éœ€è¦ä»æ–‡ä»¶åŠ è½½çœŸå®çš„å›¾ç‰‡
        try:
            raw_image = Image.new('RGB', (100, 100), color = 'red')
        except:
            raw_image = None
            print("Warning: PIL Image creation failed. Running text-only test.")


        # åˆå§‹åŒ– MLLMBot
        model_tag = "gemini-2.5-pro"
        model_name = "Gemini 2.5 Pro (API Test)"
        bot_llm = MLLMBot(model_tag=model_tag, model_name=model_name, max_answer_tokens=1024)
        
        prompt = "æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼Œæˆ–è€…å¦‚æœå›¾ç‰‡ä¸ºç©ºï¼Œè¯·å›ç­” 'å›¾ç‰‡å†…å®¹ä¸ºç©º'ã€‚"
        
        print("\n--- å‘èµ· API è°ƒç”¨ ---")
        reply, trimmed_reply = bot_llm.describe_attribute(raw_image, prompt, max_new_tokens=512)
        
        print("\n--- ç»“æœ ---")
        print(f"Model: {bot_llm.get_name()}")
        print(f"Prompt: {prompt}")
        print(f"Reply: {reply}")
        print(f"Trimmed Reply: {trimmed_reply}")
        
    except EnvironmentError as e:
        print(f"\n[é”™è¯¯]: {e}")
        print("è¯·å…ˆè®¾ç½® GEMINI_API_KEY ç¯å¢ƒå˜é‡ã€‚")
    except APIError as e:
        print(f"\n[API é”™è¯¯]: {e}")
        print("è¯·æ£€æŸ¥æ‚¨çš„ API Key æ˜¯å¦æœ‰æ•ˆï¼Œä»¥åŠæ¨¡å‹åç§°æ˜¯å¦æ­£ç¡® (gemini-2.5-pro)ã€‚")
    except Exception as e:
        print(f"\n[è¿è¡Œæ—¶é”™è¯¯]: {e}")
        
if __name__ == '__main__':
    test_mllm_output()