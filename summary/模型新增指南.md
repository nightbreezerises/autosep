# 模型新增指南

## 一、已支持的模型

| 模型名称 | 模型标识 | 本地路径 | 说明 |
|---------|---------|---------|------|
| Qwen2.5-VL-7B-Instruct | Qwen2.5-VL-7B | `models/Qwen/Qwen2.5-VL-7B-Instruct` | 默认模型 |
| Qwen3-VL-8B-Instruct | Qwen3-VL-8B | `models/Qwen/Qwen3-VL-8B-Instruct` | 备选模型 |

## 二、模型加载机制

### 2.1 单例模式

系统使用单例模式加载模型，避免重复加载：

```python
# api_utils.py
_mllm_bot_instance = None  # 全局缓存
_current_model_name = None  # 当前模型名称

def _get_mllm_bot(model_name=None):
    global _mllm_bot_instance, _current_model_name
    
    # 默认从环境变量获取模型名称
    if model_name is None:
        model_name = os.environ.get('AUTOSEP_MODEL', 'Qwen2.5-VL-7B-Instruct')
    
    # 如果已加载且名称匹配，直接返回
    if _mllm_bot_instance is not None and _current_model_name == model_name:
        return _mllm_bot_instance
    
    # 否则加载新模型...
```

### 2.2 环境变量配置

通过 `AUTOSEP_MODEL` 环境变量指定模型：

```bash
export AUTOSEP_MODEL="Qwen2.5-VL-7B-Instruct"
# 或
export AUTOSEP_MODEL="Qwen3-VL-8B-Instruct"
```

脚本会自动从 `config.yaml` 读取并设置此环境变量。

### 2.3 模型配置表

在 `api_utils.py` 中定义支持的模型：

```python
SUPPORTED_MODELS = {
    'Qwen2.5-VL-7B-Instruct': {
        'module': 'agents.mllm_bot_qwen_2_5_vl',  # 模型实现模块
        'model_tag': 'Qwen2.5-VL-7B',              # 模型标识
        'model_name': 'Qwen2.5-VL-7B-Instruct',    # 模型全名
    },
    'Qwen3-VL-8B-Instruct': {
        'module': 'agents.mllm_bot_qwen_3_vl',
        'model_tag': 'Qwen3-VL-8B',
        'model_name': 'Qwen3-VL-8B-Instruct',
    },
}
```

## 三、新增模型步骤

### 步骤 1: 下载模型权重

将模型权重放到 `models/` 目录：

```bash
# 目录结构
models/
└── Qwen/
    ├── Qwen2.5-VL-7B-Instruct/   # 已有
    ├── Qwen3-VL-8B-Instruct/     # 已有
    └── YourNewModel/              # 新模型
```

### 步骤 2: 创建模型实现文件

在 `agents/` 目录下创建新的模型实现文件，如 `mllm_bot_your_model.py`：

```python
# agents/mllm_bot_your_model.py

import torch
from PIL import Image
from transformers import YourModelClass, AutoProcessor

class MLLMBot:
    """
    模型封装类
    
    必须实现以下方法:
    - __init__: 初始化模型
    - describe_attribute: 图文推理
    - call_llm: 纯文本推理（可选）
    """
    
    def __init__(self, model_tag, model_name, pai_enable_attn=False, 
                 device='cpu', device_id=0, bit8=False, max_answer_tokens=-1):
        """
        初始化模型
        
        Args:
            model_tag: 模型标识（用于查找本地路径）
            model_name: 模型全名
            pai_enable_attn: 是否启用注意力增强
            device: 设备类型 ('cpu' 或 'cuda')
            device_id: GPU 设备 ID
            bit8: 是否使用 8bit 量化
            max_answer_tokens: 最大生成 token 数
        """
        self.model_tag = model_tag
        self.model_name = model_name
        self.max_answer_tokens = max_answer_tokens
        
        # 加载模型
        local_model_path = f"./models/Qwen/{model_name}"
        
        if device == 'cpu':
            self.device = 'cpu'
            self.model = YourModelClass.from_pretrained(local_model_path)
        else:
            self.device = f'cuda:{device_id}'
            self.model = YourModelClass.from_pretrained(
                local_model_path,
                device_map="auto",
                torch_dtype=torch.float16
            ).eval()
        
        self.processor = AutoProcessor.from_pretrained(local_model_path)
    
    def describe_attribute(self, raw_image, attr_prompt, max_new_tokens=256):
        """
        图文推理（核心方法）
        
        Args:
            raw_image: PIL.Image 对象
            attr_prompt: 文本提示
            max_new_tokens: 最大生成 token 数
        
        Returns:
            tuple: (完整回复, 清理后的回复)
        """
        # 构造输入
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": raw_image},
                    {"type": "text", "text": attr_prompt}
                ]
            }
        ]
        
        # 处理输入
        inputs = self.processor(messages, return_tensors="pt").to(self.device)
        
        # 生成
        with torch.no_grad():
            outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens)
        
        # 解码
        reply = self.processor.decode(outputs[0], skip_special_tokens=True)
        trimmed_reply = reply.strip()
        
        return reply, trimmed_reply
    
    def call_llm(self, prompts):
        """
        纯文本推理（可选）
        
        Args:
            prompts: 文本提示
        
        Returns:
            str: 生成的文本
        """
        inputs = self.processor(None, prompts, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(**inputs)
        
        return self.processor.decode(outputs[0], skip_special_tokens=True)
    
    def get_name(self):
        """返回模型名称"""
        return self.model_name
```

### 步骤 3: 注册模型

在 `api_utils.py` 的 `SUPPORTED_MODELS` 中添加新模型：

```python
SUPPORTED_MODELS = {
    # ... 已有模型
    'YourNewModel': {
        'module': 'agents.mllm_bot_your_model',  # 模块路径
        'model_tag': 'YourModel-Tag',             # 模型标识
        'model_name': 'YourNewModel',             # 模型全名
    },
}
```

### 步骤 4: 更新配置文件

在 `scripts/config.yaml` 中添加模型选项：

```yaml
# 模型名称
# 支持: Qwen2.5-VL-7B-Instruct (默认), Qwen3-VL-8B-Instruct, YourNewModel
model: YourNewModel
```

### 步骤 5: 测试新模型

```bash
# 修改配置
vim scripts/config.yaml
# 设置 model: YourNewModel

# 运行测试
bash scripts/run_pipeline.sh
```

## 四、MLLMBot 接口规范

### 4.1 必须实现的方法

| 方法 | 签名 | 说明 |
|-----|------|------|
| `__init__` | `(model_tag, model_name, pai_enable_attn, device, device_id, bit8, max_answer_tokens)` | 初始化模型 |
| `describe_attribute` | `(raw_image, attr_prompt, max_new_tokens) -> (reply, trimmed_reply)` | 图文推理 |

### 4.2 可选实现的方法

| 方法 | 签名 | 说明 |
|-----|------|------|
| `call_llm` | `(prompts) -> str` | 纯文本推理 |
| `get_name` | `() -> str` | 返回模型名称 |
| `cleanup` | `() -> None` | 清理 GPU 内存 |

### 4.3 输入输出格式

**图文推理输入**:
- `raw_image`: `PIL.Image` 对象
- `attr_prompt`: 字符串，文本提示

**图文推理输出**:
- `reply`: 字符串，模型原始输出
- `trimmed_reply`: 字符串，清理后的输出（去除多余空白等）

## 五、注意事项

### 5.1 显存管理

- GPU 模型使用单例模式，避免重复加载
- 推理后及时清理临时变量
- 可启用 8bit 量化减少显存占用

### 5.2 多进程兼容

由于 GPU 模型无法在多进程间共享，`GenericDatasetLoader` 的评估方法使用顺序执行：

```python
# data/dataset_loader.py
def run_evaluate(self, predictor, prompt, exs, ...):
    # 本地模型使用顺序执行（GPU 模型无法多进程）
    for ex in tqdm(exs, desc='评估中'):
        pred = predictor.inference(pred_prompt, [img_path], attr)
        ...
```

### 5.3 图片尺寸限制

为防止显存溢出，建议限制输入图片最大尺寸：

```python
pre_define_max_size = 1440  # 最大边长

def _resize_image_if_needed(self, image, max_size=pre_define_max_size):
    width, height = image.size
    if max(width, height) > max_size:
        scale = max_size / max(width, height)
        return image.resize((int(width*scale), int(height*scale)))
    return image
```

## 六、常见问题

### Q1: 模型加载失败？

检查以下几点：
1. 模型权重是否完整下载到 `models/` 目录
2. `SUPPORTED_MODELS` 中的 `module` 路径是否正确
3. 模型实现类名是否为 `MLLMBot`

### Q2: 显存不足？

尝试以下方法：
1. 启用 8bit 量化：`bit8=True`
2. 减小图片最大尺寸：`pre_define_max_size = 1024`
3. 减小 `max_new_tokens`

### Q3: 推理速度慢？

优化建议：
1. 使用 `torch.no_grad()` 包裹推理代码
2. 启用 KV 缓存：`use_cache=True`
3. 禁用 beam search：`num_beams=1`
